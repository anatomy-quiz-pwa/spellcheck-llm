import streamlit as st
import pandas as pd
import re, io, os, json
from pypdf import PdfReader
from datetime import datetime
import tempfile
import subprocess

# ---- Optional dependencies ----
try:
    from pdf2image import convert_from_bytes
    import pytesseract
    HAVE_OCR = True
except Exception:
    HAVE_OCR = False

try:
    from rapidfuzz import process, fuzz
    HAVE_RF = True
except Exception:
    HAVE_RF = False

try:
    import gspread
    from google.oauth2.service_account import Credentials
    HAVE_GS = True
except Exception:
    HAVE_GS = False

try:
    # æª¢æŸ¥ffmpegæ˜¯å¦å¯ç”¨
    subprocess.run(["ffmpeg", "-version"], capture_output=True, check=True)
    HAVE_FFMPEG = True
except (subprocess.CalledProcessError, FileNotFoundError):
    HAVE_FFMPEG = False

# ---- Text utilities ----
ZERO_WIDTH = "".join(["\u200b","\u200c","\u200d","\ufeff","\u00ad"])
def normalize_text(s: str) -> str:
    if not s: return ""
    s=(s.replace('ï¼ˆ','(').replace('ï¼‰',')')
         .replace('ï¼›',';').replace('ï¼Œ',',')
         .replace('ã€‚','.').replace('ã€','/'))
    for ch in ZERO_WIDTH: s=s.replace(ch,'')
    s=re.sub(r'\s+',' ',s)
    return s

def is_cjk(ch: str) -> bool:
    return '\u4e00'<=ch<='\u9fff' or '\u3400'<=ch<='\u4dbf' or '\uf900'<=ch<='\ufaff'

# ---- Termbase schema ----
REQUIRED_COLS = ["en_canonical","zh_canonical","abbr","first_mention_style","variant (éŒ¯èª¤ç”¨æ³•)","status","added_date","ç¿»è­¯ä¾†æº"]
def standardize_master(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=REQUIRED_COLS)
    for c in REQUIRED_COLS:
        if c not in df.columns: df[c] = ""
        df[c] = df[c].astype(str).fillna("").str.strip()
    df.loc[df["first_mention_style"]=="","first_mention_style"] = "ZH(EN;ABBR)"
    df.loc[df["status"]=="","status"] = "å·²ç¢ºèª"
    df.loc[df["ç¿»è­¯ä¾†æº"]=="","ç¿»è­¯ä¾†æº"] = "æ‰‹å‹•è¼¸å…¥"
    df = df.drop_duplicates(subset=["en_canonical","zh_canonical","abbr"])
    return df[REQUIRED_COLS]

# ---- Google Sheets backend ----
def extract_sheet_id(url_or_id: str) -> str:
    m = re.search(r"/spreadsheets/d/([a-zA-Z0-9-_]+)", url_or_id)
    return m.group(1) if m else url_or_id.strip()

def open_worksheet(creds_json: dict, url_or_id: str, ws_name: str):
    scopes = ["https://www.googleapis.com/auth/spreadsheets"]
    creds = Credentials.from_service_account_info(creds_json, scopes=scopes)
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(extract_sheet_id(url_or_id))
    try:
        ws = sh.worksheet(ws_name)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(title=ws_name, rows=1000, cols=10)
        ws.update([REQUIRED_COLS])
    return ws

def read_master_from_ws(ws) -> pd.DataFrame:
    values = ws.get_all_values()
    if not values: return pd.DataFrame(columns=REQUIRED_COLS)
    df = pd.DataFrame(values[1:], columns=values[0])
    return standardize_master(df)

def write_master_to_ws(ws, df: pd.DataFrame):
    df = standardize_master(df)
    values = [df.columns.tolist()] + df.values.tolist()
    ws.clear(); ws.update(values)

# ---- PDF bilingual extraction ----
def extract_text_from_images(pdf_bytes, pages_text):
    """å¾PDFåœ–ç‰‡ä¸­æå–æ–‡å­—"""
    if not HAVE_OCR:
        return pages_text
    
    try:
        # è½‰æ›PDFç‚ºåœ–ç‰‡
        images = convert_from_bytes(pdf_bytes, fmt="png")
        
        # å°æ¯ä¸€é é€²è¡ŒOCR
        for page_num, image in enumerate(images):
            try:
                # ä½¿ç”¨OCRæå–æ–‡å­—
                ocr_text = pytesseract.image_to_string(image, lang="chi_tra+eng")
                ocr_text = normalize_text(ocr_text)
                
                # å¦‚æœOCRæå–çš„æ–‡å­—æ¯”åŸå§‹æ–‡å­—å¤šï¼Œå‰‡ä½¿ç”¨OCRæ–‡å­—
                if len(ocr_text) > len(pages_text[page_num]):
                    pages_text[page_num] = ocr_text
                    
            except Exception as e:
                print(f"OCRè™•ç†ç¬¬{page_num+1}é æ™‚å‡ºéŒ¯: {e}")
                continue
                
    except Exception as e:
        print(f"åœ–ç‰‡è½‰æ›å¤±æ•—: {e}")
    
    return pages_text

def parse_pdf_pairs_with_location(pages_text: list) -> pd.DataFrame:
    """å¾PDFæ–‡å­—ä¸­æå–ä¸­è‹±æ–‡å°ç…§ï¼Œä¸¦è¨˜éŒ„ä½ç½®ä¿¡æ¯"""
    ZH = r"[ä¸€-é¾¥]{2,30}"
    EN = r"[A-Za-z][A-Za-z0-9\-\s]{1,80}"
    ABBR = r"[A-Za-z][A-Za-z0-9\-]{1,10}"

    pairs=[]
    # æ¨¡å¼1: ä¸­æ–‡(è‹±æ–‡;ç¸®å¯«)
    pat1 = re.compile(rf"(?P<zh>{ZH})\s*[\(ï¼ˆ]\s*(?P<en>{EN})(?:\s*;\s*|ï¼›\s*)(?P<abbr>{ABBR})\s*[\)ï¼‰]")
    # æ¨¡å¼2: ä¸­æ–‡(è‹±æ–‡)
    pat2 = re.compile(rf"(?P<zh>{ZH})\s*[\(ï¼ˆ]\s*(?P<en>{EN})\s*[\)ï¼‰]")
    # æ¨¡å¼3: è‹±æ–‡(ä¸­æ–‡)
    pat3 = re.compile(rf"(?P<en>{EN})\s*[\(ï¼ˆ]\s*(?P<zh>{ZH})\s*[\)ï¼‰]")
    # æ¨¡å¼4: è‹±æ–‡ - ä¸­æ–‡
    pat4 = re.compile(rf"(?P<en>{EN})\s*[-ï¼]\s*(?P<zh>{ZH})")
    # æ¨¡å¼5: ä¸­æ–‡ - è‹±æ–‡
    pat5 = re.compile(rf"(?P<zh>{ZH})\s*[-ï¼]\s*(?P<en>{EN})")

    # éæ­·æ¯ä¸€é 
    for page_num, page_text in enumerate(pages_text, 1):
        # æ¨¡å¼1: ä¸­æ–‡(è‹±æ–‡;ç¸®å¯«)
        for m in pat1.finditer(page_text):
            zh=m.group("zh").strip(); en=re.sub(r"\s+"," ", m.group("en").strip()); abbr=m.group("abbr").strip()
            context = page_text[max(0, m.start()-50):m.end()+50]
            pairs.append((en, zh, abbr, "ZH(EN;ABBR)", "", page_num, m.start(), context))
        
        # æ¨¡å¼2: ä¸­æ–‡(è‹±æ–‡)
        for m in pat2.finditer(page_text):
            zh=m.group("zh").strip(); en=re.sub(r"\s+"," ", m.group("en").strip())
            context = page_text[max(0, m.start()-50):m.end()+50]
            pairs.append((en, zh, "", "ZH(EN)", "", page_num, m.start(), context))
        
        # æ¨¡å¼3: è‹±æ–‡(ä¸­æ–‡)
        for m in pat3.finditer(page_text):
            en=re.sub(r"\s+"," ", m.group("en").strip()); zh=m.group("zh").strip()
            context = page_text[max(0, m.start()-50):m.end()+50]
            pairs.append((en, zh, "", "ZH(EN)", "", page_num, m.start(), context))
        
        # æ¨¡å¼4: è‹±æ–‡ - ä¸­æ–‡
        for m in pat4.finditer(page_text):
            en=re.sub(r"\s+"," ", m.group("en").strip()); zh=m.group("zh").strip()
            context = page_text[max(0, m.start()-50):m.end()+50]
            pairs.append((en, zh, "", "ZH(EN)", "", page_num, m.start(), context))
        
        # æ¨¡å¼5: ä¸­æ–‡ - è‹±æ–‡
        for m in pat5.finditer(page_text):
            zh=m.group("zh").strip(); en=re.sub(r"\s+"," ", m.group("en").strip())
            context = page_text[max(0, m.start()-50):m.end()+50]
            pairs.append((en, zh, "", "ZH(EN)", "", page_num, m.start(), context))

    # å‰µå»ºDataFrameï¼ŒåŒ…å«ä½ç½®ä¿¡æ¯
    df = pd.DataFrame(pairs, columns=["en_canonical", "zh_canonical", "abbr", "first_mention_style", "variant (éŒ¯èª¤ç”¨æ³•)", "page", "position", "context"])
    if df.empty: 
        df = pd.DataFrame(columns=["en_canonical", "zh_canonical", "abbr", "first_mention_style", "variant (éŒ¯èª¤ç”¨æ³•)", "page", "position", "context"])
    
    # æ·»åŠ ç¿»è­¯ä¾†æºæ¬„ä½
    df["ç¿»è­¯ä¾†æº"] = "PDFè‡ªå‹•æå–"
    
    df = df.drop_duplicates(subset=["en_canonical","zh_canonical","abbr"])
    return df

def detect_image_text_inconsistencies(pages_text: list, termbase_df: pd.DataFrame) -> list:
    """æª¢æ¸¬åœ–ç‰‡æ–‡å­—èˆ‡è©åº«çš„ä¸ä¸€è‡´æ€§"""
    if termbase_df.empty:
        return []
    
    inconsistencies = []
    
    # æå–è©åº«ä¸­çš„æ‰€æœ‰è‹±æ–‡å’Œä¸­æ–‡è©å½™
    en_terms = set(termbase_df["en_canonical"].str.lower().tolist())
    zh_terms = set(termbase_df["zh_canonical"].tolist())
    
    # è‹±æ–‡å–®è©æ¨¡å¼
    en_word_pattern = re.compile(r'\b[A-Za-z][A-Za-z0-9\-\s]{1,20}\b')
    # ä¸­æ–‡è©å½™æ¨¡å¼
    zh_word_pattern = re.compile(r'[ä¸€-é¾¥]{2,10}')
    
    for page_num, page_text in enumerate(pages_text, 1):
        # æª¢æ¸¬è‹±æ–‡è©å½™
        for match in en_word_pattern.finditer(page_text):
            word = match.group().strip()
            word_lower = word.lower()
            
            # æª¢æŸ¥æ˜¯å¦åœ¨è©åº«ä¸­
            if word_lower in en_terms:
                # æ‰¾åˆ°å°æ‡‰çš„ä¸­æ–‡ç¿»è­¯
                term_row = termbase_df[termbase_df["en_canonical"].str.lower() == word_lower]
                if not term_row.empty:
                    expected_zh = term_row.iloc[0]["zh_canonical"]
                    
                    # æª¢æŸ¥é™„è¿‘æ˜¯å¦æœ‰ä¸­æ–‡ç¿»è­¯
                    context_start = max(0, match.start() - 100)
                    context_end = min(len(page_text), match.end() + 100)
                    context = page_text[context_start:context_end]
                    
                    # æª¢æŸ¥ä¸Šä¸‹æ–‡ä¸­æ˜¯å¦åŒ…å«é æœŸçš„ä¸­æ–‡ç¿»è­¯
                    if expected_zh not in context:
                        inconsistencies.append({
                            "type": "åœ–ç‰‡è‹±æ–‡ç¼ºå°‘ä¸­æ–‡ç¿»è­¯",
                            "page": page_num,
                            "position": match.start(),
                            "english_word": word,
                            "expected_chinese": expected_zh,
                            "context": context
                        })
        
        # æª¢æ¸¬ä¸­æ–‡è©å½™
        for match in zh_word_pattern.finditer(page_text):
            zh_word = match.group().strip()
            
            # æª¢æŸ¥æ˜¯å¦åœ¨è©åº«ä¸­
            if zh_word in zh_terms:
                # æ‰¾åˆ°å°æ‡‰çš„è‹±æ–‡ç¿»è­¯
                term_row = termbase_df[termbase_df["zh_canonical"] == zh_word]
                if not term_row.empty:
                    expected_en = term_row.iloc[0]["en_canonical"]
                    
                    # æª¢æŸ¥é™„è¿‘æ˜¯å¦æœ‰è‹±æ–‡ç¿»è­¯
                    context_start = max(0, match.start() - 100)
                    context_end = min(len(page_text), match.end() + 100)
                    context = page_text[context_start:context_end]
                    
                    # æª¢æŸ¥ä¸Šä¸‹æ–‡ä¸­æ˜¯å¦åŒ…å«é æœŸçš„è‹±æ–‡ç¿»è­¯
                    if expected_en.lower() not in context.lower():
                        inconsistencies.append({
                            "type": "åœ–ç‰‡ä¸­æ–‡ç¼ºå°‘è‹±æ–‡ç¿»è­¯",
                            "page": page_num,
                            "position": match.start(),
                            "chinese_word": zh_word,
                            "expected_english": expected_en,
                            "context": context
                        })
    
    return inconsistencies

# ---- Video subtitle extraction ----
def extract_subtitles_from_video(video_bytes, video_format="mp4"):
    """å¾å½±ç‰‡ä¸­æå–å­—å¹•"""
    if not HAVE_FFMPEG:
        return [], "âŒ éœ€è¦å®‰è£ ffmpeg ä¾†è™•ç†å½±ç‰‡"
    
    try:
        # å‰µå»ºè‡¨æ™‚æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix=f".{video_format}", delete=False) as temp_video:
            temp_video.write(video_bytes)
            temp_video_path = temp_video.name
        
        # ä½¿ç”¨ffmpegæå–å­—å¹•
        subtitle_path = temp_video_path.replace(f".{video_format}", ".srt")
        
        # å˜—è©¦æå–å…§åµŒå­—å¹•
        cmd = [
            "ffmpeg", "-i", temp_video_path,
            "-map", "0:s:0",  # æå–ç¬¬ä¸€å€‹å­—å¹•è»Œé“
            "-c:s", "srt",
            subtitle_path,
            "-y"  # è¦†è“‹ç¾æœ‰æ–‡ä»¶
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0 and os.path.exists(subtitle_path):
            # æˆåŠŸæå–å­—å¹•
            with open(subtitle_path, 'r', encoding='utf-8') as f:
                subtitle_content = f.read()
            
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            os.unlink(temp_video_path)
            os.unlink(subtitle_path)
            
            return parse_srt_subtitles(subtitle_content), None
        else:
            # å˜—è©¦ä½¿ç”¨èªéŸ³è­˜åˆ¥ï¼ˆéœ€è¦é¡å¤–çš„ä¾è³´ï¼‰
            return extract_audio_and_recognize(video_bytes, temp_video_path)
            
    except Exception as e:
        return [], f"âŒ å½±ç‰‡è™•ç†å¤±æ•—ï¼š{str(e)}"

def parse_srt_subtitles(srt_content):
    """è§£æSRTå­—å¹•æ ¼å¼"""
    subtitles = []
    lines = srt_content.strip().split('\n')
    
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        
        # è·³éç©ºè¡Œ
        if not line:
            i += 1
            continue
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå­—å¹•åºè™Ÿ
        if line.isdigit():
            # è®€å–æ™‚é–“æˆ³
            if i + 1 < len(lines):
                timestamp = lines[i + 1].strip()
                i += 2
                
                # è®€å–å­—å¹•æ–‡æœ¬
                subtitle_text = []
                while i < len(lines) and lines[i].strip():
                    subtitle_text.append(lines[i].strip())
                    i += 1
                
                if subtitle_text:
                    subtitles.append({
                        'text': ' '.join(subtitle_text),
                        'timestamp': timestamp,
                        'start_time': parse_timestamp(timestamp.split(' --> ')[0]),
                        'end_time': parse_timestamp(timestamp.split(' --> ')[1]) if ' --> ' in timestamp else None
                    })
        i += 1
    
    return subtitles

def parse_timestamp(timestamp):
    """è§£ææ™‚é–“æˆ³æ ¼å¼ (HH:MM:SS,mmm)"""
    try:
        time_part, ms_part = timestamp.split(',')
        h, m, s = map(int, time_part.split(':'))
        ms = int(ms_part)
        return h * 3600 + m * 60 + s + ms / 1000
    except:
        return 0

def extract_audio_and_recognize(video_bytes, video_path):
    """æå–éŸ³é »ä¸¦é€²è¡ŒèªéŸ³è­˜åˆ¥ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
    try:
        # æå–éŸ³é »
        audio_path = video_path.replace('.mp4', '.wav')
        cmd = [
            "ffmpeg", "-i", video_path,
            "-vn", "-acodec", "pcm_s16le",
            "-ar", "16000", "-ac", "1",
            audio_path,
            "-y"
        ]
        
        result = subprocess.run(cmd, capture_output=True)
        
        if result.returncode == 0:
            # é€™è£¡å¯ä»¥æ·»åŠ èªéŸ³è­˜åˆ¥åŠŸèƒ½
            # ç›®å‰è¿”å›ç©ºåˆ—è¡¨ï¼Œè¡¨ç¤ºéœ€è¦æ‰‹å‹•è¼¸å…¥å­—å¹•
            os.unlink(video_path)
            os.unlink(audio_path)
            return [], "â„¹ï¸ å½±ç‰‡æ²’æœ‰å…§åµŒå­—å¹•ï¼Œè«‹æ‰‹å‹•è¼¸å…¥å­—å¹•å…§å®¹"
        else:
            os.unlink(video_path)
            return [], "âŒ éŸ³é »æå–å¤±æ•—"
            
    except Exception as e:
        return [], f"âŒ éŸ³é »è™•ç†å¤±æ•—ï¼š{str(e)}"

def detect_subtitle_inconsistencies(subtitles, termbase_df):
    """æª¢æ¸¬å­—å¹•ä¸­çš„ç¿»è­¯ä¸ä¸€è‡´æ€§"""
    if termbase_df.empty or not subtitles:
        return []
    
    inconsistencies = []
    
    # æå–è©åº«ä¸­çš„æ‰€æœ‰è‹±æ–‡å’Œä¸­æ–‡è©å½™
    en_terms = set(termbase_df["en_canonical"].str.lower().tolist())
    zh_terms = set(termbase_df["zh_canonical"].tolist())
    
    # è‹±æ–‡å–®è©æ¨¡å¼
    en_word_pattern = re.compile(r'\b[A-Za-z][A-Za-z0-9\-\s]{1,20}\b')
    # ä¸­æ–‡è©å½™æ¨¡å¼
    zh_word_pattern = re.compile(r'[ä¸€-é¾¥]{2,10}')
    
    for subtitle in subtitles:
        text = subtitle['text']
        timestamp = subtitle['timestamp']
        
        # æª¢æ¸¬è‹±æ–‡è©å½™
        for match in en_word_pattern.finditer(text):
            word = match.group().strip()
            word_lower = word.lower()
            
            # æª¢æŸ¥æ˜¯å¦åœ¨è©åº«ä¸­
            if word_lower in en_terms:
                # æ‰¾åˆ°å°æ‡‰çš„ä¸­æ–‡ç¿»è­¯
                term_row = termbase_df[termbase_df["en_canonical"].str.lower() == word_lower]
                if not term_row.empty:
                    expected_zh = term_row.iloc[0]["zh_canonical"]
                    
                    # æª¢æŸ¥å­—å¹•ä¸­æ˜¯å¦åŒ…å«é æœŸçš„ä¸­æ–‡ç¿»è­¯
                    if expected_zh not in text:
                        inconsistencies.append({
                            "type": "å­—å¹•è‹±æ–‡ç¼ºå°‘ä¸­æ–‡ç¿»è­¯",
                            "timestamp": timestamp,
                            "english_word": word,
                            "expected_chinese": expected_zh,
                            "subtitle_text": text
                        })
        
        # æª¢æ¸¬ä¸­æ–‡è©å½™
        for match in zh_word_pattern.finditer(text):
            zh_word = match.group().strip()
            
            # æª¢æŸ¥æ˜¯å¦åœ¨è©åº«ä¸­
            if zh_word in zh_terms:
                # æ‰¾åˆ°å°æ‡‰çš„è‹±æ–‡ç¿»è­¯
                term_row = termbase_df[termbase_df["zh_canonical"] == zh_word]
                if not term_row.empty:
                    expected_en = term_row.iloc[0]["en_canonical"]
                    
                    # æª¢æŸ¥å­—å¹•ä¸­æ˜¯å¦åŒ…å«é æœŸçš„è‹±æ–‡ç¿»è­¯
                    if expected_en.lower() not in text.lower():
                        inconsistencies.append({
                            "type": "å­—å¹•ä¸­æ–‡ç¼ºå°‘è‹±æ–‡ç¿»è­¯",
                            "timestamp": timestamp,
                            "chinese_word": zh_word,
                            "expected_english": expected_en,
                            "subtitle_text": text
                        })
    
    return inconsistencies

def parse_pdf_pairs(full_text: str) -> pd.DataFrame:
    """å¾PDFæ–‡å­—ä¸­æå–ä¸­è‹±æ–‡å°ç…§ï¼ˆä¿æŒå‘å¾Œå…¼å®¹ï¼‰"""
    ZH = r"[ä¸€-é¾¥]{2,30}"
    EN = r"[A-Za-z][A-Za-z0-9\-\s]{1,80}"
    ABBR = r"[A-Za-z][A-Za-z0-9\-]{1,10}"

    pairs=[]
    # æ¨¡å¼1: ä¸­æ–‡(è‹±æ–‡;ç¸®å¯«)
    pat1 = re.compile(rf"(?P<zh>{ZH})\s*[\(ï¼ˆ]\s*(?P<en>{EN})(?:\s*;\s*|ï¼›\s*)(?P<abbr>{ABBR})\s*[\)ï¼‰]")
    # æ¨¡å¼2: ä¸­æ–‡(è‹±æ–‡)
    pat2 = re.compile(rf"(?P<zh>{ZH})\s*[\(ï¼ˆ]\s*(?P<en>{EN})\s*[\)ï¼‰]")
    # æ¨¡å¼3: è‹±æ–‡(ä¸­æ–‡)
    pat3 = re.compile(rf"(?P<en>{EN})\s*[\(ï¼ˆ]\s*(?P<zh>{ZH})\s*[\)ï¼‰]")
    # æ¨¡å¼4: è‹±æ–‡ - ä¸­æ–‡
    pat4 = re.compile(rf"(?P<en>{EN})\s*[-ï¼]\s*(?P<zh>{ZH})")
    # æ¨¡å¼5: ä¸­æ–‡ - è‹±æ–‡
    pat5 = re.compile(rf"(?P<zh>{ZH})\s*[-ï¼]\s*(?P<en>{EN})")

    for m in pat1.finditer(full_text):
        zh=m.group("zh").strip(); en=re.sub(r"\s+"," ", m.group("en").strip()); abbr=m.group("abbr").strip()
        pairs.append((en, zh, abbr, "ZH(EN;ABBR)", ""))
    for m in pat2.finditer(full_text):
        zh=m.group("zh").strip(); en=re.sub(r"\s+"," ", m.group("en").strip())
        pairs.append((en, zh, "", "ZH(EN)", ""))
    for m in pat3.finditer(full_text):
        en=re.sub(r"\s+"," ", m.group("en").strip()); zh=m.group("zh").strip()
        pairs.append((en, zh, "", "ZH(EN)", ""))
    for m in pat4.finditer(full_text):
        en=re.sub(r"\s+"," ", m.group("en").strip()); zh=m.group("zh").strip()
        pairs.append((en, zh, "", "ZH(EN)", ""))
    for m in pat5.finditer(full_text):
        zh=m.group("zh").strip(); en=re.sub(r"\s+"," ", m.group("en").strip())
        pairs.append((en, zh, "", "ZH(EN)", ""))

    # å‰µå»ºDataFrameï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„åˆ—ï¼ˆé™¤äº†statuså’Œadded_dateï¼‰
    df = pd.DataFrame(pairs, columns=["en_canonical", "zh_canonical", "abbr", "first_mention_style", "variant (éŒ¯èª¤ç”¨æ³•)"])
    if df.empty: 
        df = pd.DataFrame(columns=["en_canonical", "zh_canonical", "abbr", "first_mention_style", "variant (éŒ¯èª¤ç”¨æ³•)"])
    
    # æ·»åŠ ç¿»è­¯ä¾†æºæ¬„ä½
    df["ç¿»è­¯ä¾†æº"] = "PDFè‡ªå‹•æå–"
    
    df = df.drop_duplicates(subset=["en_canonical","zh_canonical","abbr"])
    return df

def detect_differences(extracted_df: pd.DataFrame, termbase_df: pd.DataFrame) -> tuple:
    """æª¢æ¸¬æå–çš„å…§å®¹èˆ‡è©åº«çš„å·®ç•°"""
    # æ¨™æº–åŒ–å…©å€‹DataFrame
    extracted_clean = extracted_df[["en_canonical","zh_canonical","abbr"]].copy()
    termbase_clean = termbase_df[["en_canonical","zh_canonical","abbr"]].copy()
    
    # æ‰¾å‡ºæ–°å¢çš„å…§å®¹ï¼ˆåœ¨æå–ä¸­ä½†ä¸åœ¨è©åº«ä¸­ï¼‰
    merged = extracted_clean.merge(termbase_clean, on=["en_canonical","zh_canonical","abbr"], how="left", indicator=True)
    new_items = merged[merged["_merge"]=="left_only"][["en_canonical","zh_canonical","abbr"]]
    
    # æ‰¾å‡ºå¯èƒ½çš„éŒ¯èª¤ï¼ˆåœ¨è©åº«ä¸­ä½†èˆ‡æå–çš„å…§å®¹ä¸ä¸€è‡´ï¼‰
    potential_errors = []
    
    # æª¢æŸ¥è‹±æ–‡ç›¸åŒä½†ä¸­æ–‡ä¸åŒçš„æƒ…æ³
    en_merge = extracted_clean.merge(termbase_clean, on="en_canonical", how="inner", suffixes=("_ext", "_term"))
    en_conflicts = en_merge[en_merge["zh_canonical_ext"] != en_merge["zh_canonical_term"]]
    
    # æª¢æŸ¥ä¸­æ–‡ç›¸åŒä½†è‹±æ–‡ä¸åŒçš„æƒ…æ³
    zh_merge = extracted_clean.merge(termbase_clean, on="zh_canonical", how="inner", suffixes=("_ext", "_term"))
    zh_conflicts = zh_merge[zh_merge["en_canonical_ext"] != zh_merge["en_canonical_term"]]
    
    for _, row in en_conflicts.iterrows():
        potential_errors.append({
            "type": "è‹±æ–‡ç›¸åŒï¼Œä¸­æ–‡ä¸åŒ",
            "en_canonical": row["en_canonical"],
            "zh_extracted": row["zh_canonical_ext"],
            "zh_termbase": row["zh_canonical_term"],
            "abbr_extracted": row["abbr_ext"],
            "abbr_termbase": row["abbr_term"]
        })
    
    for _, row in zh_conflicts.iterrows():
        potential_errors.append({
            "type": "ä¸­æ–‡ç›¸åŒï¼Œè‹±æ–‡ä¸åŒ",
            "zh_canonical": row["zh_canonical"],
            "en_extracted": row["en_canonical_ext"],
            "en_termbase": row["en_canonical_term"],
            "abbr_extracted": row["abbr_ext"],
            "abbr_termbase": row["abbr_term"]
        })
    
    return new_items, potential_errors

def detect_differences_with_location(extracted_df_with_location: pd.DataFrame, termbase_df: pd.DataFrame) -> tuple:
    """æª¢æ¸¬æå–çš„å…§å®¹èˆ‡è©åº«çš„å·®ç•°ï¼ˆåŒ…å«ä½ç½®ä¿¡æ¯ï¼‰"""
    # æ¨™æº–åŒ–å…©å€‹DataFrame
    extracted_clean = extracted_df_with_location[["en_canonical","zh_canonical","abbr"]].copy()
    termbase_clean = termbase_df[["en_canonical","zh_canonical","abbr"]].copy()
    
    # æ‰¾å‡ºæ–°å¢çš„å…§å®¹ï¼ˆåœ¨æå–ä¸­ä½†ä¸åœ¨è©åº«ä¸­ï¼‰
    merged = extracted_clean.merge(termbase_clean, on=["en_canonical","zh_canonical","abbr"], how="left", indicator=True)
    new_items = merged[merged["_merge"]=="left_only"][["en_canonical","zh_canonical","abbr"]]
    
    # æ‰¾å‡ºå¯èƒ½çš„éŒ¯èª¤ï¼ˆåœ¨è©åº«ä¸­ä½†èˆ‡æå–çš„å…§å®¹ä¸ä¸€è‡´ï¼‰
    potential_errors = []
    
    # æª¢æŸ¥è‹±æ–‡ç›¸åŒä½†ä¸­æ–‡ä¸åŒçš„æƒ…æ³
    en_merge = extracted_clean.merge(termbase_clean, on="en_canonical", how="inner", suffixes=("_ext", "_term"))
    en_conflicts = en_merge[en_merge["zh_canonical_ext"] != en_merge["zh_canonical_term"]]
    
    # æª¢æŸ¥ä¸­æ–‡ç›¸åŒä½†è‹±æ–‡ä¸åŒçš„æƒ…æ³
    zh_merge = extracted_clean.merge(termbase_clean, on="zh_canonical", how="inner", suffixes=("_ext", "_term"))
    zh_conflicts = zh_merge[zh_merge["en_canonical_ext"] != zh_merge["en_canonical_term"]]
    
    for _, row in en_conflicts.iterrows():
        # æ‰¾åˆ°å°æ‡‰çš„ä½ç½®ä¿¡æ¯
        matching_row = extracted_df_with_location[
            (extracted_df_with_location["en_canonical"] == row["en_canonical"]) &
            (extracted_df_with_location["zh_canonical"] == row["zh_canonical_ext"])
        ]
        
        if not matching_row.empty:
            location_row = matching_row.iloc[0]
            
            # æª¢æŸ¥è©åº«ä¸­å°æ‡‰æ¢ç›®çš„ç‹€æ…‹
            termbase_row = termbase_df[termbase_df["en_canonical"] == row["en_canonical"]]
            status = termbase_row.iloc[0]["status"] if not termbase_row.empty else "æœªçŸ¥"
            
            potential_errors.append({
                "type": "è‹±æ–‡ç›¸åŒï¼Œä¸­æ–‡ä¸åŒ",
                "en_canonical": row["en_canonical"],
                "zh_extracted": row["zh_canonical_ext"],
                "zh_termbase": row["zh_canonical_term"],
                "abbr_extracted": row["abbr_ext"],
                "abbr_termbase": row["abbr_term"],
                "termbase_status": status,
                "page": location_row["page"],
                "position": location_row["position"],
                "context": location_row["context"]
            })
    
    for _, row in zh_conflicts.iterrows():
        # æ‰¾åˆ°å°æ‡‰çš„ä½ç½®ä¿¡æ¯
        matching_row = extracted_df_with_location[
            (extracted_df_with_location["zh_canonical"] == row["zh_canonical"]) &
            (extracted_df_with_location["en_canonical"] == row["en_canonical_ext"])
        ]
        
        if not matching_row.empty:
            location_row = matching_row.iloc[0]
            
            # æª¢æŸ¥è©åº«ä¸­å°æ‡‰æ¢ç›®çš„ç‹€æ…‹
            termbase_row = termbase_df[termbase_df["zh_canonical"] == row["zh_canonical"]]
            status = termbase_row.iloc[0]["status"] if not termbase_row.empty else "æœªçŸ¥"
            
            potential_errors.append({
                "type": "ä¸­æ–‡ç›¸åŒï¼Œè‹±æ–‡ä¸åŒ",
                "zh_canonical": row["zh_canonical"],
                "en_extracted": row["en_canonical_ext"],
                "en_termbase": row["en_canonical_term"],
                "abbr_extracted": row["abbr_ext"],
                "abbr_termbase": row["abbr_term"],
                "termbase_status": status,
                "page": location_row["page"],
                "position": location_row["position"],
                "context": location_row["context"]
            })
    
    return new_items, potential_errors

# ---- Streamlit UI ----
st.set_page_config(page_title="PDFè‡ªå‹•æå–ä¸­è‹±æ–‡ç¿»è­¯å°æ¯”ç³»çµ±", layout="wide")
st.title("ğŸ“„ PDFè‡ªå‹•æå–ä¸­è‹±æ–‡ç¿»è­¯å°æ¯”ç³»çµ±")
st.markdown("**åŠŸèƒ½ï¼š** è‡ªå‹•æå–PDFä¸­çš„ä¸­è‹±æ–‡ç¿»è­¯ï¼Œå°æ¯”Google Sheetsè©åº«ï¼Œæª¢æ¸¬å·®ç•°ä¸¦è‡ªå‹•æ–°å¢")

# ç°¡åŒ–çš„ä½¿ç”¨èªªæ˜
with st.expander("ğŸ“– å¿«é€Ÿä½¿ç”¨èªªæ˜", expanded=False):
    st.markdown("""
    ### ğŸš€ ä¸‰æ­¥é©Ÿå®Œæˆï¼š
    1. **ä¸Šå‚³PDF** - é¸æ“‡åŒ…å«ä¸­è‹±æ–‡å°ç…§çš„æŠ•å½±ç‰‡
    2. **è‡ªå‹•è™•ç†** - ç³»çµ±è‡ªå‹•æå–ä¸¦å°æ¯”è©åº«
    3. **ä¸€éµæ–°å¢** - é»æ“ŠæŒ‰éˆ•è‡ªå‹•æ–°å¢åˆ°Google Sheets
    
    ### ğŸ“‹ æ”¯æ´çš„æ ¼å¼ï¼š
    - `ä¸­æ–‡(è‹±æ–‡;ç¸®å¯«)`
    - `ä¸­æ–‡(è‹±æ–‡)`
    - `è‹±æ–‡(ä¸­æ–‡)`
    - `è‹±æ–‡ - ä¸­æ–‡`
    - `ä¸­æ–‡ - è‹±æ–‡`
    
    ### âš™ï¸ è‡ªå‹•åŒ–åŠŸèƒ½ï¼š
    - è‡ªå‹•é€£ç·šGoogle Sheets
    - è‡ªå‹•æå–ä¸­è‹±æ–‡å°ç…§
    - è‡ªå‹•æª¢æ¸¬æ–°å¢å…§å®¹
    - è‡ªå‹•æ¨™è¨˜ç‚º"æ–°å¢å¾…ç¢ºèª"
    """)

# é è¨­è¨­å®š
DEFAULT_SHEET_URL = "https://docs.google.com/spreadsheets/d/1UbBCtcUscJ65lCBkjLmZod3L7lR5e85ztgyPcetwGWs/edit?usp=sharing"
DEFAULT_WS_NAME = "termbase_master"
DEFAULT_JSON_PATH = "service_account_key.json"

# å´é‚Šæ¬„é…ç½®
with st.sidebar:
    st.subheader("ğŸ”— Google Sheets è¨­å®š")
    use_gs = st.toggle("ä½¿ç”¨ Google Sheets", value=True)
    
    # è‡ªå‹•å¡«å…¥é è¨­å€¼
    sheet_url = st.text_input("Sheet URL æˆ– ID", value=DEFAULT_SHEET_URL, disabled=not use_gs)
    ws_name = st.text_input("å·¥ä½œè¡¨åç¨±", value=DEFAULT_WS_NAME, disabled=not use_gs)
    
    # é¡¯ç¤ºç•¶å‰ä½¿ç”¨çš„ JSON æ–‡ä»¶
    if os.path.exists(DEFAULT_JSON_PATH):
        st.success(f"âœ… ä½¿ç”¨é è¨­ JSON: {DEFAULT_JSON_PATH}")
    else:
        st.error(f"âŒ æ‰¾ä¸åˆ°é è¨­ JSON: {DEFAULT_JSON_PATH}")
        creds_file = st.file_uploader("ä¸Šå‚³ service account JSON", type=["json"], disabled=not use_gs)

    st.subheader("ğŸ” OCR è¨­å®š")
    ocr_enabled = st.toggle("å•Ÿç”¨ OCR å¾Œå‚™", value=True)
    ocr_thresh = st.slider("OCR è§¸ç™¼ï¼šæŠ½å–å­—å…ƒå°‘æ–¼", 0, 200, 10, disabled=not ocr_enabled)
    ocr_lang = st.text_input("OCR èªè¨€", value="chi_tra+eng", disabled=not ocr_enabled)

    st.subheader("âš™ï¸ è‡ªå‹•åŒ–è¨­å®š")
    auto_add_new = st.toggle("è‡ªå‹•æ–°å¢æ–°å…§å®¹", value=True)
    auto_mark_pending = st.toggle("è‡ªå‹•æ¨™è¨˜ç‚ºå¾…ç¢ºèª", value=True)

# è¼‰å…¥è©åº«
if use_gs:
    if not HAVE_GS:
        st.error("âŒ æœªå®‰è£ gspread / google-authï¼š`pip install gspread google-auth`")
        st.stop()
    
    # è‡ªå‹•ä½¿ç”¨é è¨­ JSON æ–‡ä»¶
    if os.path.exists(DEFAULT_JSON_PATH):
        try:
            with open(DEFAULT_JSON_PATH, 'r', encoding='utf-8') as f:
                creds_json = json.load(f)
            ws = open_worksheet(creds_json, sheet_url, ws_name)
            termbase = read_master_from_ws(ws)
            st.success(f"âœ… å·²è‡ªå‹•é€£ç·šï¼š{ws.spreadsheet.title} / {ws.title}")
        except Exception as e:
            st.error(f"âŒ è‡ªå‹•é€£ç·š Google Sheets å¤±æ•—ï¼š{e}")
            st.info("â„¹ï¸ è«‹æª¢æŸ¥ JSON æ–‡ä»¶æˆ–æ‰‹å‹•ä¸Šå‚³æ†‘è­‰ã€‚")
            ws = None
            termbase = standardize_master(pd.DataFrame())
    else:
        st.error(f"âŒ æ‰¾ä¸åˆ°é è¨­ JSON æ–‡ä»¶ï¼š{DEFAULT_JSON_PATH}")
        st.info("â„¹ï¸ è«‹æ‰‹å‹•ä¸Šå‚³ JSON æ†‘è­‰æ–‡ä»¶ã€‚")
        ws = None
        termbase = standardize_master(pd.DataFrame())
else:
    ws = None
    termbase = standardize_master(pd.DataFrame())

# é¡¯ç¤ºè©åº«çµ±è¨ˆ
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("è©åº«ç¸½æ¢ç›®", len(termbase))
with col2:
    confirmed_count = len(termbase[termbase["status"] == "å·²ç¢ºèª"])
    st.metric("å·²ç¢ºèªæ¢ç›®", confirmed_count)
with col3:
    pending_count = len(termbase[termbase["status"] == "æ–°å¢å¾…ç¢ºèª"])
    st.metric("å¾…ç¢ºèªæ¢ç›®", pending_count)
with col4:
    pdf_source_count = len(termbase[termbase["ç¿»è­¯ä¾†æº"] == "PDFè‡ªå‹•æå–"])
    st.metric("PDFä¾†æº", pdf_source_count)

# ä¸»è¦åŠŸèƒ½å€åŸŸ
st.write("---")
st.subheader("ğŸš€ å¿«é€Ÿé–‹å§‹ï¼šä¸Šå‚³æ–‡ä»¶")

# é¸æ“‡æ–‡ä»¶é¡å‹
file_type = st.radio(
    "é¸æ“‡æ–‡ä»¶é¡å‹ï¼š",
    ["ğŸ“„ PDF æŠ•å½±ç‰‡", "ğŸ¬ å½±ç‰‡æ–‡ä»¶"],
    horizontal=True
)

if file_type == "ğŸ“„ PDF æŠ•å½±ç‰‡":
    # ä¸Šå‚³PDF
    pdf_file = st.file_uploader("ğŸ“¤ ä¸Šå‚³æŠ•å½±ç‰‡ PDF", type=["pdf"], help="é¸æ“‡åŒ…å«ä¸­è‹±æ–‡å°ç…§çš„PDFæŠ•å½±ç‰‡æ–‡ä»¶")
    video_file = None
else:
    # ä¸Šå‚³å½±ç‰‡
    video_file = st.file_uploader("ğŸ“¤ ä¸Šå‚³å½±ç‰‡æ–‡ä»¶", type=["mp4", "avi", "mov", "mkv"], help="é¸æ“‡åŒ…å«å­—å¹•çš„å½±ç‰‡æ–‡ä»¶")
    pdf_file = None
    
    # é¡¯ç¤ºffmpegç‹€æ…‹
    if not HAVE_FFMPEG:
        st.warning("âš ï¸ éœ€è¦å®‰è£ ffmpeg ä¾†è™•ç†å½±ç‰‡æ–‡ä»¶")
        st.info("å®‰è£æ–¹æ³•ï¼š`brew install ffmpeg` (macOS) æˆ– `sudo apt install ffmpeg` (Ubuntu)")
    else:
        st.success("âœ… ffmpeg å·²å®‰è£ï¼Œå¯ä»¥è™•ç†å½±ç‰‡æ–‡ä»¶")

if pdf_file or video_file:
    if pdf_file:
        with st.spinner("æ­£åœ¨è™•ç†PDF..."):
            pdf_bytes = pdf_file.read()
            
            # æå–PDFæ–‡å­—ï¼ˆåŒ…å«OCRå¾Œå‚™ï¼‰
            reader = PdfReader(io.BytesIO(pdf_bytes))
            pages_text = []
            
            for pno, page in enumerate(reader.pages, start=1):
                try:
                    raw = page.extract_text() or ""
                except Exception:
                    raw = ""
                
                norm = normalize_text(raw)
                
                # å¦‚æœæ–‡å­—å¤ªå°‘ä¸”å•Ÿç”¨OCRï¼Œå˜—è©¦OCR
                if ocr_enabled and len(norm) < ocr_thresh and HAVE_OCR:
                    try:
                        images = convert_from_bytes(pdf_bytes, first_page=pno, last_page=pno, fmt="png")
                        if images:
                            txt = pytesseract.image_to_string(images[0], lang=ocr_lang)
                            norm = normalize_text(txt)
                    except Exception:
                        pass
                
                pages_text.append(norm)
            
            # åœ–ç‰‡æ–‡å­—æƒæï¼ˆå¢å¼·OCRï¼‰
            if ocr_enabled and HAVE_OCR:
                pages_text = extract_text_from_images(pdf_bytes, pages_text)
            
            full_text = "\n".join(pages_text)
            
            # æå–ä¸­è‹±æ–‡å°ç…§ï¼ˆå¸¶ä½ç½®ä¿¡æ¯ï¼‰
            extracted_pairs_with_location = parse_pdf_pairs_with_location(pages_text)
            extracted_pairs = parse_pdf_pairs(full_text)  # ä¿æŒå‘å¾Œå…¼å®¹
            
            # æª¢æ¸¬å·®ç•°ï¼ˆä½¿ç”¨å¸¶ä½ç½®ä¿¡æ¯çš„å‡½æ•¸ï¼‰
            new_items, potential_errors = detect_differences_with_location(extracted_pairs_with_location, termbase)
            
            # æª¢æ¸¬åœ–ç‰‡æ–‡å­—ä¸€è‡´æ€§
            image_inconsistencies = detect_image_text_inconsistencies(pages_text, termbase)
            
            # å½±ç‰‡ç›¸é—œè®Šæ•¸è¨­ç‚ºç©º
            subtitles = []
            subtitle_inconsistencies = []
            
    elif video_file:
        with st.spinner("æ­£åœ¨è™•ç†å½±ç‰‡..."):
            video_bytes = video_file.read()
            video_format = video_file.name.split('.')[-1].lower()
            
            # æå–å­—å¹•
            subtitles, error_msg = extract_subtitles_from_video(video_bytes, video_format)
            
            if error_msg:
                st.warning(error_msg)
                
                # æä¾›æ‰‹å‹•è¼¸å…¥å­—å¹•çš„é¸é …
                st.subheader("ğŸ“ æ‰‹å‹•è¼¸å…¥å­—å¹•")
                manual_subtitles = st.text_area(
                    "è«‹è¼¸å…¥å­—å¹•å…§å®¹ï¼ˆæ¯è¡Œä¸€å€‹å­—å¹•ï¼‰ï¼š",
                    height=200,
                    help="æ ¼å¼ï¼šæ™‚é–“æˆ³ å­—å¹•å…§å®¹ï¼Œä¾‹å¦‚ï¼š00:01:30,000 --> 00:01:35,000 é€™æ˜¯å­—å¹•å…§å®¹"
                )
                
                if manual_subtitles:
                    # ç°¡å–®è§£ææ‰‹å‹•è¼¸å…¥çš„å­—å¹•
                    lines = manual_subtitles.strip().split('\n')
                    subtitles = []
                    for line in lines:
                        if ' --> ' in line:
                            parts = line.split(' --> ')
                            if len(parts) == 2:
                                timestamp = line
                                text = parts[1].split(' ', 1)[1] if len(parts[1].split(' ', 1)) > 1 else ""
                                subtitles.append({
                                    'text': text,
                                    'timestamp': timestamp,
                                    'start_time': parse_timestamp(parts[0]),
                                    'end_time': parse_timestamp(parts[1])
                                })
            
            # æª¢æ¸¬å­—å¹•ä¸ä¸€è‡´æ€§
            subtitle_inconsistencies = detect_subtitle_inconsistencies(subtitles, termbase)
            
            # å¾å­—å¹•ä¸­æå–ä¸­è‹±æ–‡å°ç…§
            subtitle_text = "\n".join([sub['text'] for sub in subtitles])
            extracted_pairs_with_location = parse_pdf_pairs_with_location([subtitle_text])
            extracted_pairs = parse_pdf_pairs(subtitle_text)
            
            # æª¢æ¸¬å·®ç•°
            new_items, potential_errors = detect_differences_with_location(extracted_pairs_with_location, termbase)
            
            # PDFç›¸é—œè®Šæ•¸è¨­ç‚ºç©º
            pages_text = []
            image_inconsistencies = []
        
        # é¡¯ç¤ºçµæœ
        st.success(f"âœ… è™•ç†å®Œæˆï¼")
        
        # çµ±è¨ˆè³‡è¨Š
        if pdf_file:
            col1, col2, col3, col4, col5 = st.columns(5)
            with col1:
                st.metric("æå–çš„å°ç…§", len(extracted_pairs))
            with col2:
                st.metric("æ–°å¢å…§å®¹", len(new_items))
            with col3:
                st.metric("æ½›åœ¨éŒ¯èª¤", len(potential_errors))
            with col4:
                st.metric("åœ–ç‰‡æ–‡å­—å•é¡Œ", len(image_inconsistencies))
            with col5:
                st.metric("PDFé æ•¸", len(pages_text))
        else:
            col1, col2, col3, col4, col5 = st.columns(5)
            with col1:
                st.metric("æå–çš„å°ç…§", len(extracted_pairs))
            with col2:
                st.metric("æ–°å¢å…§å®¹", len(new_items))
            with col3:
                st.metric("æ½›åœ¨éŒ¯èª¤", len(potential_errors))
            with col4:
                st.metric("å­—å¹•å•é¡Œ", len(subtitle_inconsistencies))
            with col5:
                st.metric("å­—å¹•æ•¸é‡", len(subtitles))
        
        # å¿«é€Ÿä¸Šå‚³æŒ‰éˆ•ï¼ˆå¦‚æœæœ‰æ–°å…§å®¹ï¼‰
        if not new_items.empty:
            st.write("---")
            st.subheader("ğŸš€ å¿«é€Ÿä¸Šå‚³")
            
            # æª¢æŸ¥éé‡è¤‡å…§å®¹
            non_duplicate_count = 0
            for _, new_row in new_items.iterrows():
                existing_match = termbase[
                    (termbase["en_canonical"] == new_row["en_canonical"]) |
                    (termbase["zh_canonical"] == new_row["zh_canonical"])
                ]
                if existing_match.empty:
                    non_duplicate_count += 1
            
            if non_duplicate_count > 0:
                st.success(f"ğŸ¯ ç™¼ç¾ {non_duplicate_count} æ¢éé‡è¤‡å…§å®¹å¯ä¸Šå‚³")
                
                if st.button("ğŸ“¤ ç«‹å³ä¸Šå‚³åˆ° Google Sheets", type="primary", use_container_width=True, help="é»æ“Šæ­¤æŒ‰éˆ•å¿«é€Ÿä¸Šå‚³éé‡è¤‡å…§å®¹"):
                    if ws is not None:
                        try:
                            # æº–å‚™éé‡è¤‡çš„æ–°å…§å®¹
                            new_data = []
                            current_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            
                            for _, row in new_items.iterrows():
                                # æª¢æŸ¥æ˜¯å¦é‡è¤‡
                                existing_match = termbase[
                                    (termbase["en_canonical"] == row["en_canonical"]) |
                                    (termbase["zh_canonical"] == row["zh_canonical"])
                                ]
                                
                                if existing_match.empty:  # åªæœ‰éé‡è¤‡çš„æ‰æ·»åŠ 
                                    new_row = {
                                        "en_canonical": row["en_canonical"],
                                        "zh_canonical": row["zh_canonical"],
                                        "abbr": row["abbr"],
                                        "first_mention_style": "ZH(EN;ABBR)",
                                        "variant (éŒ¯èª¤ç”¨æ³•)": "",
                                        "status": "æ–°å¢å¾…ç¢ºèª" if auto_mark_pending else "å·²ç¢ºèª",
                                        "added_date": current_date,
                                        "ç¿»è­¯ä¾†æº": "PDFè‡ªå‹•æå–"
                                    }
                                    new_data.append(new_row)
                            
                            if new_data:
                                new_df = pd.DataFrame(new_data)
                                # åˆä½µç¾æœ‰è©åº«å’Œæ–°å…§å®¹
                                combined = pd.concat([termbase, new_df], ignore_index=True)
                                combined = standardize_master(combined)
                                
                                # å¯«å…¥Google Sheets
                                write_master_to_ws(ws, combined)
                                st.success(f"ğŸ‰ æˆåŠŸä¸Šå‚³ {len(new_df)} æ¢å…§å®¹åˆ° Google Sheetsï¼")
                                
                                # æ›´æ–°æœ¬åœ°è©åº«
                                termbase = combined
                                
                                # é¡¯ç¤ºæ…¶ç¥æ•ˆæœ
                                st.balloons()
                                
                                # é‡æ–°è¼‰å…¥é é¢
                                st.rerun()
                            else:
                                st.warning("âš ï¸ æ²’æœ‰éé‡è¤‡å…§å®¹å¯ä¸Šå‚³")
                                
                        except Exception as e:
                            st.error(f"âŒ ä¸Šå‚³å¤±æ•—ï¼š{e}")
                            st.error("è«‹æª¢æŸ¥ Google Sheets é€£ç·šå’Œæ¬Šé™è¨­å®š")
                    else:
                        st.error("âŒ ç„¡æ³•é€£ç·šåˆ° Google Sheets")
            else:
                st.info("â„¹ï¸ æ‰€æœ‰å…§å®¹éƒ½èˆ‡ç¾æœ‰è©åº«é‡è¤‡ï¼Œç„¡éœ€ä¸Šå‚³")
        
        # åˆ†é é¡¯ç¤ºçµæœ
        if pdf_file:
            tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["ğŸ“‹ æå–çš„å°ç…§", "ğŸ” éŒ¯å­—æª¢æ¸¬", "ğŸ–¼ï¸ åœ–ç‰‡æ–‡å­—æª¢æ¸¬", "ğŸ†• æ–°å¢å…§å®¹", "âš ï¸ æ½›åœ¨éŒ¯èª¤", "ğŸ“„ åŸå§‹æ–‡å­—"])
        else:
            tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["ğŸ“‹ æå–çš„å°ç…§", "ğŸ” éŒ¯å­—æª¢æ¸¬", "ğŸ“º å­—å¹•æª¢æ¸¬", "ğŸ†• æ–°å¢å…§å®¹", "âš ï¸ æ½›åœ¨éŒ¯èª¤", "ğŸ“„ å­—å¹•å…§å®¹"])
        
        with tab1:
            st.subheader("ğŸ“‹ å¾PDFä¸­æå–çš„ä¸­è‹±æ–‡å°ç…§ï¼ˆå«ä½ç½®ä¿¡æ¯ï¼‰")
            if not extracted_pairs_with_location.empty:
                # é¡¯ç¤ºå¸¶ä½ç½®ä¿¡æ¯çš„è¡¨æ ¼
                display_df = extracted_pairs_with_location[["en_canonical", "zh_canonical", "abbr", "page", "position", "context"]].copy()
                display_df.columns = ["è‹±æ–‡", "ä¸­æ–‡", "ç¸®å¯«", "é ç¢¼", "ä½ç½®", "ä¸Šä¸‹æ–‡"]
                st.dataframe(display_df, use_container_width=True)
                
                # éŒ¯å­—æª¢æŸ¥åŠŸèƒ½
                st.write("---")
                st.subheader("ğŸ” éŒ¯å­—æª¢æŸ¥")
                
                # è®“ç”¨æˆ¶é¸æ“‡è¦æª¢æŸ¥çš„é …ç›®
                if not extracted_pairs_with_location.empty:
                    selected_items = st.multiselect(
                        "é¸æ“‡è¦æª¢æŸ¥çš„é …ç›®ï¼š",
                        options=[f"ç¬¬{row['page']}é : {row['zh_canonical']} ({row['en_canonical']})" 
                                for _, row in extracted_pairs_with_location.iterrows()],
                        help="é¸æ“‡æ‚¨æ‡·ç–‘æœ‰éŒ¯å­—çš„é …ç›®é€²è¡Œæª¢æŸ¥"
                    )
                    
                    if selected_items:
                        st.write("### ğŸ“ é¸ä¸­é …ç›®çš„è©³ç´°ä½ç½®ä¿¡æ¯ï¼š")
                        for item in selected_items:
                            # è§£æé¸ä¸­çš„é …ç›®
                            page_match = re.search(r"ç¬¬(\d+)é :", item)
                            zh_match = re.search(r": (.+?) \(", item)
                            en_match = re.search(r"\((.+?)\)", item)
                            
                            if page_match and zh_match and en_match:
                                page_num = int(page_match.group(1))
                                zh_text = zh_match.group(1)
                                en_text = en_match.group(1)
                                
                                # æ‰¾åˆ°å°æ‡‰çš„è¡Œ
                                matching_row = extracted_pairs_with_location[
                                    (extracted_pairs_with_location["page"] == page_num) &
                                    (extracted_pairs_with_location["zh_canonical"] == zh_text) &
                                    (extracted_pairs_with_location["en_canonical"] == en_text)
                                ]
                                
                                if not matching_row.empty:
                                    row = matching_row.iloc[0]
                                    st.write(f"**ğŸ“ ä½ç½®ï¼šç¬¬ {row['page']} é ï¼Œä½ç½® {row['position']}**")
                                    st.write(f"**ğŸ“ å…§å®¹ï¼š{row['zh_canonical']} ({row['en_canonical']})**")
                                    st.write(f"**ğŸ“„ ä¸Šä¸‹æ–‡ï¼š**")
                                    st.code(row['context'], language="text")
                                    st.write("---")
            else:
                st.info("æœªæ‰¾åˆ°ä¸­è‹±æ–‡å°ç…§")
        
        with tab2:
            st.subheader("ğŸ†• æ–°å¢å…§å®¹ï¼ˆè©åº«ä¸­æ²’æœ‰çš„ï¼‰")
            if not new_items.empty:
                st.info(f"ğŸ¯ ç™¼ç¾ {len(new_items)} å€‹æ–°å…§å®¹ï¼Œéœ€è¦æ·»åŠ åˆ°è©åº«")
                
                # æº–å‚™æ–°å¢çš„è³‡æ–™
                new_data = []
                current_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                
                for _, row in new_items.iterrows():
                    new_row = {
                        "en_canonical": row["en_canonical"],
                        "zh_canonical": row["zh_canonical"],
                        "abbr": row["abbr"],
                        "first_mention_style": "ZH(EN;ABBR)",
                        "variant (éŒ¯èª¤ç”¨æ³•)": "",
                        "status": "æ–°å¢å¾…ç¢ºèª" if auto_mark_pending else "å·²ç¢ºèª",
                        "added_date": current_date,
                        "ç¿»è­¯ä¾†æº": "PDFè‡ªå‹•æå–"
                    }
                    new_data.append(new_row)
                
                new_df = pd.DataFrame(new_data)
                
                # é¡¯ç¤ºæ–°å¢å…§å®¹
                st.write("ğŸ“‹ **å³å°‡æ–°å¢çš„å…§å®¹ï¼š**")
                st.dataframe(new_df, use_container_width=True)
                
                # æª¢æŸ¥é‡è¤‡å…§å®¹
                st.write("ğŸ” **é‡è¤‡æª¢æŸ¥ï¼š**")
                duplicate_check = []
                for _, new_row in new_df.iterrows():
                    # æª¢æŸ¥æ˜¯å¦èˆ‡ç¾æœ‰è©åº«é‡è¤‡
                    existing_match = termbase[
                        (termbase["en_canonical"] == new_row["en_canonical"]) |
                        (termbase["zh_canonical"] == new_row["zh_canonical"])
                    ]
                    
                    if not existing_match.empty:
                        duplicate_check.append({
                            "æ–°å…§å®¹": f"{new_row['zh_canonical']} ({new_row['en_canonical']})",
                            "é‡è¤‡é …ç›®": f"{existing_match.iloc[0]['zh_canonical']} ({existing_match.iloc[0]['en_canonical']})",
                            "ç‹€æ…‹": "âš ï¸ é‡è¤‡"
                        })
                    else:
                        duplicate_check.append({
                            "æ–°å…§å®¹": f"{new_row['zh_canonical']} ({new_row['en_canonical']})",
                            "é‡è¤‡é …ç›®": "ç„¡",
                            "ç‹€æ…‹": "âœ… å¯æ–°å¢"
                        })
                
                duplicate_df = pd.DataFrame(duplicate_check)
                st.dataframe(duplicate_df, use_container_width=True)
                
                # éæ¿¾æ‰é‡è¤‡çš„å…§å®¹
                non_duplicate_df = new_df.copy()
                for _, new_row in new_df.iterrows():
                    existing_match = termbase[
                        (termbase["en_canonical"] == new_row["en_canonical"]) |
                        (termbase["zh_canonical"] == new_row["zh_canonical"])
                    ]
                    if not existing_match.empty:
                        non_duplicate_df = non_duplicate_df[
                            ~((non_duplicate_df["en_canonical"] == new_row["en_canonical"]) &
                              (non_duplicate_df["zh_canonical"] == new_row["zh_canonical"]))
                        ]
                
                if not non_duplicate_df.empty:
                    st.success(f"âœ… éæ¿¾å¾Œæœ‰ {len(non_duplicate_df)} æ¢éé‡è¤‡å…§å®¹å¯æ–°å¢")
                    
                    # ä¸»è¦ä¸Šå‚³æŒ‰éˆ•
                    st.write("---")
                    st.subheader("ğŸš€ ä¸Šå‚³åˆ° Google Sheets")
                    
                    col1, col2 = st.columns([2, 1])
                    with col1:
                        if st.button("ğŸ“¤ ä¸€éµä¸Šå‚³åˆ° Google Sheets", type="primary", use_container_width=True, help="é»æ“Šæ­¤æŒ‰éˆ•å°‡éé‡è¤‡å…§å®¹è‡ªå‹•ä¸Šå‚³åˆ° Google Sheets"):
                            if ws is not None:
                                try:
                                    # åˆä½µç¾æœ‰è©åº«å’Œéé‡è¤‡æ–°å…§å®¹
                                    combined = pd.concat([termbase, non_duplicate_df], ignore_index=True)
                                    combined = standardize_master(combined)
                                    
                                    # å¯«å…¥Google Sheets
                                    write_master_to_ws(ws, combined)
                                    st.success(f"ğŸ‰ æˆåŠŸä¸Šå‚³ {len(non_duplicate_df)} æ¢å…§å®¹åˆ° Google Sheetsï¼")
                                    
                                    # æ›´æ–°æœ¬åœ°è©åº«
                                    termbase = combined
                                    
                                    # é¡¯ç¤ºæ›´æ–°å¾Œçš„çµ±è¨ˆ
                                    st.balloons()
                                    
                                    # é‡æ–°è¼‰å…¥é é¢é¡¯ç¤ºæ›´æ–°å¾Œçš„çµ±è¨ˆ
                                    st.rerun()
                                    
                                except Exception as e:
                                    st.error(f"âŒ ä¸Šå‚³å¤±æ•—ï¼š{e}")
                                    st.error("è«‹æª¢æŸ¥ Google Sheets é€£ç·šå’Œæ¬Šé™è¨­å®š")
                            else:
                                st.error("âŒ ç„¡æ³•é€£ç·šåˆ° Google Sheets")
                    
                    with col2:
                        if st.button("ğŸ”„ é‡æ–°æª¢æŸ¥", use_container_width=True):
                            st.rerun()
                else:
                    st.warning("âš ï¸ æ‰€æœ‰å…§å®¹éƒ½èˆ‡ç¾æœ‰è©åº«é‡è¤‡ï¼Œç„¡éœ€æ–°å¢")
            else:
                st.success("âœ… æ²’æœ‰ç™¼ç¾æ–°å…§å®¹")
        
        with tab2:
            st.subheader("ğŸ” éŒ¯å­—æª¢æ¸¬")
            st.write("**åŠŸèƒ½ï¼š** è‡ªå‹•æª¢æ¸¬PDFä¸­çš„ä¸­è‹±æ–‡å°ç…§æ˜¯å¦èˆ‡è©åº«ä¸€è‡´ï¼Œæ‰¾å‡ºå¯èƒ½çš„éŒ¯å­—")
            
            if not extracted_pairs_with_location.empty and not termbase.empty:
                # æª¢æ¸¬éŒ¯å­—
                typo_detections = []
                
                for _, extracted_row in extracted_pairs_with_location.iterrows():
                    # æª¢æŸ¥æ˜¯å¦åœ¨è©åº«ä¸­å­˜åœ¨
                    existing_match = termbase[
                        (termbase["en_canonical"] == extracted_row["en_canonical"]) |
                        (termbase["zh_canonical"] == extracted_row["zh_canonical"])
                    ]
                    
                    if not existing_match.empty:
                        # æª¢æŸ¥æ˜¯å¦å®Œå…¨åŒ¹é…
                        exact_match = existing_match[
                            (existing_match["en_canonical"] == extracted_row["en_canonical"]) &
                            (existing_match["zh_canonical"] == extracted_row["zh_canonical"])
                        ]
                        
                        if exact_match.empty:
                            # éƒ¨åˆ†åŒ¹é…ï¼Œå¯èƒ½æ˜¯éŒ¯å­—
                            typo_detections.append({
                                "é ç¢¼": extracted_row["page"],
                                "ä½ç½®": extracted_row["position"],
                                "PDFå…§å®¹": f"{extracted_row['zh_canonical']} ({extracted_row['en_canonical']})",
                                "è©åº«å…§å®¹": f"{existing_match.iloc[0]['zh_canonical']} ({existing_match.iloc[0]['en_canonical']})",
                                "å•é¡Œé¡å‹": "ä¸­è‹±æ–‡ä¸åŒ¹é…",
                                "ä¸Šä¸‹æ–‡": extracted_row["context"]
                            })
                    else:
                        # å®Œå…¨ä¸åœ¨è©åº«ä¸­
                        typo_detections.append({
                            "é ç¢¼": extracted_row["page"],
                            "ä½ç½®": extracted_row["position"],
                            "PDFå…§å®¹": f"{extracted_row['zh_canonical']} ({extracted_row['en_canonical']})",
                            "è©åº«å…§å®¹": "æœªæ‰¾åˆ°",
                            "å•é¡Œé¡å‹": "è©åº«ä¸­ä¸å­˜åœ¨",
                            "ä¸Šä¸‹æ–‡": extracted_row["context"]
                        })
                
                if typo_detections:
                    st.warning(f"âš ï¸ ç™¼ç¾ {len(typo_detections)} å€‹å¯èƒ½çš„éŒ¯å­—æˆ–å•é¡Œ")
                    
                    # é¡¯ç¤ºéŒ¯å­—æª¢æ¸¬çµæœ
                    typo_df = pd.DataFrame(typo_detections)
                    st.dataframe(typo_df, use_container_width=True)
                    
                    # è©³ç´°æŸ¥çœ‹åŠŸèƒ½
                    st.write("---")
                    st.subheader("ğŸ“ è©³ç´°ä½ç½®ä¿¡æ¯")
                    
                    for i, typo in enumerate(typo_detections):
                        with st.expander(f"å•é¡Œ {i+1}: ç¬¬{typo['é ç¢¼']}é  - {typo['PDFå…§å®¹']}"):
                            st.write(f"**ğŸ“ ä½ç½®ï¼š** ç¬¬ {typo['é ç¢¼']} é ï¼Œä½ç½® {typo['ä½ç½®']}")
                            st.write(f"**ğŸ“ PDFå…§å®¹ï¼š** {typo['PDFå…§å®¹']}")
                            st.write(f"**ğŸ“š è©åº«å…§å®¹ï¼š** {typo['è©åº«å…§å®¹']}")
                            st.write(f"**âš ï¸ å•é¡Œé¡å‹ï¼š** {typo['å•é¡Œé¡å‹']}")
                            st.write(f"**ğŸ“„ ä¸Šä¸‹æ–‡ï¼š**")
                            st.code(typo['ä¸Šä¸‹æ–‡'], language="text")
                            
                            # æä¾›ä¿®æ­£å»ºè­°
                            if typo['å•é¡Œé¡å‹'] == "ä¸­è‹±æ–‡ä¸åŒ¹é…":
                                st.write("**ğŸ’¡ ä¿®æ­£å»ºè­°ï¼š**")
                                st.write("1. æª¢æŸ¥ä¸­æ–‡ç¿»è­¯æ˜¯å¦æ­£ç¢º")
                                st.write("2. æª¢æŸ¥è‹±æ–‡æ‹¼å¯«æ˜¯å¦æ­£ç¢º")
                                st.write("3. ç¢ºèªæ˜¯å¦ç‚ºåŒç¾©è©æˆ–è¿‘ç¾©è©")
                            elif typo['å•é¡Œé¡å‹'] == "è©åº«ä¸­ä¸å­˜åœ¨":
                                st.write("**ğŸ’¡ ä¿®æ­£å»ºè­°ï¼š**")
                                st.write("1. æª¢æŸ¥æ˜¯å¦ç‚ºæ–°è¡“èª")
                                st.write("2. æª¢æŸ¥æ˜¯å¦æœ‰æ‹¼å¯«éŒ¯èª¤")
                                st.write("3. è€ƒæ…®æ·»åŠ åˆ°è©åº«")
                else:
                    st.success("âœ… æœªç™¼ç¾éŒ¯å­—ï¼Œæ‰€æœ‰ä¸­è‹±æ–‡å°ç…§éƒ½èˆ‡è©åº«ä¸€è‡´")
            else:
                if extracted_pairs_with_location.empty:
                    st.info("â„¹ï¸ æœªæå–åˆ°ä¸­è‹±æ–‡å°ç…§")
                if termbase.empty:
                    st.info("â„¹ï¸ è©åº«ç‚ºç©ºï¼Œç„¡æ³•é€²è¡ŒéŒ¯å­—æª¢æ¸¬")
        
        with tab3:
            if pdf_file:
                st.subheader("ğŸ–¼ï¸ åœ–ç‰‡æ–‡å­—æª¢æ¸¬")
                st.write("**åŠŸèƒ½ï¼š** æª¢æ¸¬åœ–ç‰‡ä¸­çš„æ–‡å­—æ˜¯å¦èˆ‡è©åº«ä¸€è‡´ï¼Œæ‰¾å‡ºç¼ºå°‘ç¿»è­¯çš„å…§å®¹")
                
                if image_inconsistencies:
                    st.warning(f"âš ï¸ ç™¼ç¾ {len(image_inconsistencies)} å€‹åœ–ç‰‡æ–‡å­—å•é¡Œ")
                    
                    # å‰µå»ºé¡¯ç¤ºè¡¨æ ¼
                    display_inconsistencies = []
                    for item in image_inconsistencies:
                        if item["type"] == "åœ–ç‰‡è‹±æ–‡ç¼ºå°‘ä¸­æ–‡ç¿»è­¯":
                            display_inconsistencies.append({
                                "é ç¢¼": item["page"],
                                "ä½ç½®": item["position"],
                                "å•é¡Œé¡å‹": item["type"],
                                "è‹±æ–‡è©å½™": item["english_word"],
                                "ç¼ºå°‘çš„ä¸­æ–‡": item["expected_chinese"],
                                "ä¸Šä¸‹æ–‡": item["context"][:100] + "..." if len(item["context"]) > 100 else item["context"]
                            })
                        else:
                            display_inconsistencies.append({
                                "é ç¢¼": item["page"],
                                "ä½ç½®": item["position"],
                                "å•é¡Œé¡å‹": item["type"],
                                "ä¸­æ–‡è©å½™": item["chinese_word"],
                                "ç¼ºå°‘çš„è‹±æ–‡": item["expected_english"],
                                "ä¸Šä¸‹æ–‡": item["context"][:100] + "..." if len(item["context"]) > 100 else item["context"]
                            })
                    
                    inconsistencies_df = pd.DataFrame(display_inconsistencies)
                    st.dataframe(inconsistencies_df, use_container_width=True)
                    
                    # è©³ç´°æŸ¥çœ‹åŠŸèƒ½
                    st.write("---")
                    st.subheader("ğŸ“ è©³ç´°å•é¡Œä¿¡æ¯")
                    
                    for i, item in enumerate(image_inconsistencies):
                        if item["type"] == "åœ–ç‰‡è‹±æ–‡ç¼ºå°‘ä¸­æ–‡ç¿»è­¯":
                            with st.expander(f"å•é¡Œ {i+1}: ç¬¬{item['page']}é  - {item['english_word']} ç¼ºå°‘ä¸­æ–‡ç¿»è­¯"):
                                st.write(f"**ğŸ“ ä½ç½®ï¼š** ç¬¬ {item['page']} é ï¼Œä½ç½® {item['position']}")
                                st.write(f"**ğŸ“ è‹±æ–‡è©å½™ï¼š** {item['english_word']}")
                                st.write(f"**ğŸ“š ç¼ºå°‘çš„ä¸­æ–‡ç¿»è­¯ï¼š** {item['expected_chinese']}")
                                st.write(f"**ğŸ“„ ä¸Šä¸‹æ–‡ï¼š**")
                                st.code(item['context'], language="text")
                                st.write("**ğŸ’¡ å»ºè­°ï¼š** åœ¨åœ–ç‰‡ä¸­æ·»åŠ ä¸­æ–‡ç¿»è­¯æˆ–æª¢æŸ¥æ˜¯å¦ç‚ºæ–°è¡“èª")
                        else:
                            with st.expander(f"å•é¡Œ {i+1}: ç¬¬{item['page']}é  - {item['chinese_word']} ç¼ºå°‘è‹±æ–‡ç¿»è­¯"):
                                st.write(f"**ğŸ“ ä½ç½®ï¼š** ç¬¬ {item['page']} é ï¼Œä½ç½® {item['position']}")
                                st.write(f"**ğŸ“ ä¸­æ–‡è©å½™ï¼š** {item['chinese_word']}")
                                st.write(f"**ğŸ“š ç¼ºå°‘çš„è‹±æ–‡ç¿»è­¯ï¼š** {item['expected_english']}")
                                st.write(f"**ğŸ“„ ä¸Šä¸‹æ–‡ï¼š**")
                                st.code(item['context'], language="text")
                                st.write("**ğŸ’¡ å»ºè­°ï¼š** åœ¨åœ–ç‰‡ä¸­æ·»åŠ è‹±æ–‡ç¿»è­¯æˆ–æª¢æŸ¥æ˜¯å¦ç‚ºæ–°è¡“èª")
                else:
                    st.success("âœ… åœ–ç‰‡æ–‡å­—æª¢æ¸¬å®Œæˆï¼Œæœªç™¼ç¾å•é¡Œ")
                    st.info("â„¹ï¸ æ‰€æœ‰åœ–ç‰‡ä¸­çš„æ–‡å­—éƒ½èˆ‡è©åº«ä¸€è‡´ï¼Œæˆ–è©åº«ç‚ºç©ºç„¡æ³•æª¢æ¸¬")
            else:
                st.subheader("ğŸ“º å­—å¹•æª¢æ¸¬")
                st.write("**åŠŸèƒ½ï¼š** æª¢æ¸¬å­—å¹•ä¸­çš„ç¿»è­¯æ˜¯å¦èˆ‡è©åº«ä¸€è‡´ï¼Œæ‰¾å‡ºç¼ºå°‘ç¿»è­¯çš„å…§å®¹")
                
                if subtitle_inconsistencies:
                    st.warning(f"âš ï¸ ç™¼ç¾ {len(subtitle_inconsistencies)} å€‹å­—å¹•å•é¡Œ")
                    
                    # å‰µå»ºé¡¯ç¤ºè¡¨æ ¼
                    display_inconsistencies = []
                    for item in subtitle_inconsistencies:
                        if item["type"] == "å­—å¹•è‹±æ–‡ç¼ºå°‘ä¸­æ–‡ç¿»è­¯":
                            display_inconsistencies.append({
                                "æ™‚é–“æˆ³": item["timestamp"],
                                "å•é¡Œé¡å‹": item["type"],
                                "è‹±æ–‡è©å½™": item["english_word"],
                                "ç¼ºå°‘çš„ä¸­æ–‡": item["expected_chinese"],
                                "å­—å¹•å…§å®¹": item["subtitle_text"][:100] + "..." if len(item["subtitle_text"]) > 100 else item["subtitle_text"]
                            })
                        else:
                            display_inconsistencies.append({
                                "æ™‚é–“æˆ³": item["timestamp"],
                                "å•é¡Œé¡å‹": item["type"],
                                "ä¸­æ–‡è©å½™": item["chinese_word"],
                                "ç¼ºå°‘çš„è‹±æ–‡": item["expected_english"],
                                "å­—å¹•å…§å®¹": item["subtitle_text"][:100] + "..." if len(item["subtitle_text"]) > 100 else item["subtitle_text"]
                            })
                    
                    inconsistencies_df = pd.DataFrame(display_inconsistencies)
                    st.dataframe(inconsistencies_df, use_container_width=True)
                    
                    # è©³ç´°æŸ¥çœ‹åŠŸèƒ½
                    st.write("---")
                    st.subheader("ğŸ“ è©³ç´°å•é¡Œä¿¡æ¯")
                    
                    for i, item in enumerate(subtitle_inconsistencies):
                        if item["type"] == "å­—å¹•è‹±æ–‡ç¼ºå°‘ä¸­æ–‡ç¿»è­¯":
                            with st.expander(f"å•é¡Œ {i+1}: {item['timestamp']} - {item['english_word']} ç¼ºå°‘ä¸­æ–‡ç¿»è­¯"):
                                st.write(f"**â° æ™‚é–“æˆ³ï¼š** {item['timestamp']}")
                                st.write(f"**ğŸ“ è‹±æ–‡è©å½™ï¼š** {item['english_word']}")
                                st.write(f"**ğŸ“š ç¼ºå°‘çš„ä¸­æ–‡ç¿»è­¯ï¼š** {item['expected_chinese']}")
                                st.write(f"**ğŸ“„ å­—å¹•å…§å®¹ï¼š**")
                                st.code(item['subtitle_text'], language="text")
                                st.write("**ğŸ’¡ å»ºè­°ï¼š** åœ¨å­—å¹•ä¸­æ·»åŠ ä¸­æ–‡ç¿»è­¯æˆ–æª¢æŸ¥æ˜¯å¦ç‚ºæ–°è¡“èª")
                        else:
                            with st.expander(f"å•é¡Œ {i+1}: {item['timestamp']} - {item['chinese_word']} ç¼ºå°‘è‹±æ–‡ç¿»è­¯"):
                                st.write(f"**â° æ™‚é–“æˆ³ï¼š** {item['timestamp']}")
                                st.write(f"**ğŸ“ ä¸­æ–‡è©å½™ï¼š** {item['chinese_word']}")
                                st.write(f"**ğŸ“š ç¼ºå°‘çš„è‹±æ–‡ç¿»è­¯ï¼š** {item['expected_english']}")
                                st.write(f"**ğŸ“„ å­—å¹•å…§å®¹ï¼š**")
                                st.code(item['subtitle_text'], language="text")
                                st.write("**ğŸ’¡ å»ºè­°ï¼š** åœ¨å­—å¹•ä¸­æ·»åŠ è‹±æ–‡ç¿»è­¯æˆ–æª¢æŸ¥æ˜¯å¦ç‚ºæ–°è¡“èª")
                else:
                    st.success("âœ… å­—å¹•æª¢æ¸¬å®Œæˆï¼Œæœªç™¼ç¾å•é¡Œ")
                    st.info("â„¹ï¸ æ‰€æœ‰å­—å¹•éƒ½èˆ‡è©åº«ä¸€è‡´ï¼Œæˆ–è©åº«ç‚ºç©ºç„¡æ³•æª¢æ¸¬")
        
        with tab4:
            st.subheader("ğŸ†• æ–°å¢å…§å®¹ï¼ˆè©åº«ä¸­æ²’æœ‰çš„ï¼‰")
            if not new_items.empty:
                st.info(f"ğŸ¯ ç™¼ç¾ {len(new_items)} å€‹æ–°å…§å®¹ï¼Œéœ€è¦æ·»åŠ åˆ°è©åº«")
        
        with tab5:
            st.subheader("âš ï¸ æ½›åœ¨éŒ¯èª¤ï¼ˆéœ€è¦äººå·¥æª¢æŸ¥ï¼‰")
            if potential_errors:
                # æ ¹æ“šç‹€æ…‹åˆ†é¡éŒ¯èª¤
                confirmed_errors = [e for e in potential_errors if e.get('termbase_status') == 'å·²ç¢ºèª']
                pending_errors = [e for e in potential_errors if e.get('termbase_status') == 'æ–°å¢å¾…ç¢ºèª']
                other_errors = [e for e in potential_errors if e.get('termbase_status') not in ['å·²ç¢ºèª', 'æ–°å¢å¾…ç¢ºèª']]
                
                # é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ç¸½éŒ¯èª¤æ•¸", len(potential_errors))
                with col2:
                    st.metric("å·²ç¢ºèªè©åº«éŒ¯èª¤", len(confirmed_errors), delta=f"{len(confirmed_errors)}å€‹åš´é‡éŒ¯èª¤")
                with col3:
                    st.metric("å¾…ç¢ºèªè©åº«éŒ¯èª¤", len(pending_errors), delta=f"{len(pending_errors)}å€‹å¾…å¯©æ ¸")
                
                # å„ªå…ˆé¡¯ç¤ºå·²ç¢ºèªè©åº«çš„éŒ¯èª¤ï¼ˆæœ€åš´é‡ï¼‰
                if confirmed_errors:
                    st.error(f"ğŸš¨ ç™¼ç¾ {len(confirmed_errors)} å€‹èˆ‡å·²ç¢ºèªè©åº«ä¸ç¬¦çš„åš´é‡éŒ¯èª¤ï¼")
                    
                    # å‰µå»ºå·²ç¢ºèªéŒ¯èª¤çš„é¡¯ç¤ºè¡¨æ ¼
                    display_confirmed_errors = []
                    for error in confirmed_errors:
                        if 'page' in error:
                            display_confirmed_errors.append({
                                "é ç¢¼": error['page'],
                                "ä½ç½®": error['position'],
                                "å•é¡Œé¡å‹": error['type'],
                                "è©åº«ç‹€æ…‹": "âœ… å·²ç¢ºèª",
                                "æå–å…§å®¹": f"{error.get('zh_extracted', error.get('zh_canonical', ''))} ({error.get('en_extracted', error.get('en_canonical', ''))})",
                                "è©åº«å…§å®¹": f"{error.get('zh_termbase', '')} ({error.get('en_termbase', '')})"
                            })
                        else:
                            display_confirmed_errors.append({
                                "é ç¢¼": "æœªçŸ¥",
                                "ä½ç½®": "æœªçŸ¥",
                                "å•é¡Œé¡å‹": error['type'],
                                "è©åº«ç‹€æ…‹": "âœ… å·²ç¢ºèª",
                                "æå–å…§å®¹": f"{error.get('zh_extracted', error.get('zh_canonical', ''))} ({error.get('en_extracted', error.get('en_canonical', ''))})",
                                "è©åº«å…§å®¹": f"{error.get('zh_termbase', '')} ({error.get('en_termbase', '')})"
                            })
                    
                    confirmed_errors_df = pd.DataFrame(display_confirmed_errors)
                    st.dataframe(confirmed_errors_df, use_container_width=True)
                
                # é¡¯ç¤ºå¾…ç¢ºèªè©åº«çš„éŒ¯èª¤
                if pending_errors:
                    st.warning(f"âš ï¸ ç™¼ç¾ {len(pending_errors)} å€‹èˆ‡å¾…ç¢ºèªè©åº«ä¸ç¬¦çš„éŒ¯èª¤")
                    
                    # å‰µå»ºå¾…ç¢ºèªéŒ¯èª¤çš„é¡¯ç¤ºè¡¨æ ¼
                    display_pending_errors = []
                    for error in pending_errors:
                        if 'page' in error:
                            display_pending_errors.append({
                                "é ç¢¼": error['page'],
                                "ä½ç½®": error['position'],
                                "å•é¡Œé¡å‹": error['type'],
                                "è©åº«ç‹€æ…‹": "â³ å¾…ç¢ºèª",
                                "æå–å…§å®¹": f"{error.get('zh_extracted', error.get('zh_canonical', ''))} ({error.get('en_extracted', error.get('en_canonical', ''))})",
                                "è©åº«å…§å®¹": f"{error.get('zh_termbase', '')} ({error.get('en_termbase', '')})"
                            })
                        else:
                            display_pending_errors.append({
                                "é ç¢¼": "æœªçŸ¥",
                                "ä½ç½®": "æœªçŸ¥",
                                "å•é¡Œé¡å‹": error['type'],
                                "è©åº«ç‹€æ…‹": "â³ å¾…ç¢ºèª",
                                "æå–å…§å®¹": f"{error.get('zh_extracted', error.get('zh_canonical', ''))} ({error.get('en_extracted', error.get('en_canonical', ''))})",
                                "è©åº«å…§å®¹": f"{error.get('zh_termbase', '')} ({error.get('en_termbase', '')})"
                            })
                    
                    pending_errors_df = pd.DataFrame(display_pending_errors)
                    st.dataframe(pending_errors_df, use_container_width=True)
                
                # é¡¯ç¤ºå…¶ä»–éŒ¯èª¤
                if other_errors:
                    st.info(f"â„¹ï¸ ç™¼ç¾ {len(other_errors)} å€‹å…¶ä»–ç‹€æ…‹çš„éŒ¯èª¤")
                    
                    # å‰µå»ºå…¶ä»–éŒ¯èª¤çš„é¡¯ç¤ºè¡¨æ ¼
                    display_other_errors = []
                    for error in other_errors:
                        if 'page' in error:
                            display_other_errors.append({
                                "é ç¢¼": error['page'],
                                "ä½ç½®": error['position'],
                                "å•é¡Œé¡å‹": error['type'],
                                "è©åº«ç‹€æ…‹": error.get('termbase_status', 'æœªçŸ¥'),
                                "æå–å…§å®¹": f"{error.get('zh_extracted', error.get('zh_canonical', ''))} ({error.get('en_extracted', error.get('en_canonical', ''))})",
                                "è©åº«å…§å®¹": f"{error.get('zh_termbase', '')} ({error.get('en_termbase', '')})"
                            })
                        else:
                            display_other_errors.append({
                                "é ç¢¼": "æœªçŸ¥",
                                "ä½ç½®": "æœªçŸ¥",
                                "å•é¡Œé¡å‹": error['type'],
                                "è©åº«ç‹€æ…‹": error.get('termbase_status', 'æœªçŸ¥'),
                                "æå–å…§å®¹": f"{error.get('zh_extracted', error.get('zh_canonical', ''))} ({error.get('en_extracted', error.get('en_canonical', ''))})",
                                "è©åº«å…§å®¹": f"{error.get('zh_termbase', '')} ({error.get('en_termbase', '')})"
                            })
                    
                    other_errors_df = pd.DataFrame(display_other_errors)
                    st.dataframe(other_errors_df, use_container_width=True)
                
                # æä¾›ä¿®æ­£é¸é …
                st.write("---")
                st.subheader("ğŸ“ è©³ç´°ä½ç½®ä¿¡æ¯")
                for i, error in enumerate(potential_errors):
                    if 'page' in error:
                        with st.expander(f"éŒ¯èª¤ {i+1}: ç¬¬{error['page']}é  - {error['type']}"):
                            st.write(f"**ğŸ“ ä½ç½®ï¼š** ç¬¬ {error['page']} é ï¼Œä½ç½® {error['position']}")
                            
                            if error['type'] == "è‹±æ–‡ç›¸åŒï¼Œä¸­æ–‡ä¸åŒ":
                                st.write(f"**ğŸ“ è‹±æ–‡:** {error['en_canonical']}")
                                st.write(f"**ğŸ“ æå–çš„ä¸­æ–‡:** {error['zh_extracted']}")
                                st.write(f"**ğŸ“š è©åº«ä¸­çš„ä¸­æ–‡:** {error['zh_termbase']}")
                            else:
                                st.write(f"**ğŸ“ ä¸­æ–‡:** {error['zh_canonical']}")
                                st.write(f"**ğŸ“ æå–çš„è‹±æ–‡:** {error['en_extracted']}")
                                st.write(f"**ğŸ“š è©åº«ä¸­çš„è‹±æ–‡:** {error['en_termbase']}")
                            
                            if 'context' in error:
                                st.write(f"**ğŸ“„ ä¸Šä¸‹æ–‡ï¼š**")
                                st.code(error['context'], language="text")
                            
                            st.write("**ğŸ’¡ è«‹æª¢æŸ¥å“ªå€‹ç‰ˆæœ¬æ˜¯æ­£ç¢ºçš„**")
                    else:
                        with st.expander(f"éŒ¯èª¤ {i+1}: {error['type']}"):
                            if error['type'] == "è‹±æ–‡ç›¸åŒï¼Œä¸­æ–‡ä¸åŒ":
                                st.write(f"**ğŸ“ è‹±æ–‡:** {error['en_canonical']}")
                                st.write(f"**ğŸ“ æå–çš„ä¸­æ–‡:** {error['zh_extracted']}")
                                st.write(f"**ğŸ“š è©åº«ä¸­çš„ä¸­æ–‡:** {error['zh_termbase']}")
                            else:
                                st.write(f"**ğŸ“ ä¸­æ–‡:** {error['zh_canonical']}")
                                st.write(f"**ğŸ“ æå–çš„è‹±æ–‡:** {error['en_extracted']}")
                                st.write(f"**ğŸ“š è©åº«ä¸­çš„è‹±æ–‡:** {error['en_termbase']}")
                            
                            st.write("**ğŸ’¡ è«‹æª¢æŸ¥å“ªå€‹ç‰ˆæœ¬æ˜¯æ­£ç¢ºçš„**")
            else:
                st.success("âœ… æ²’æœ‰ç™¼ç¾æ½›åœ¨éŒ¯èª¤")
        
        with tab6:
            if pdf_file:
                st.subheader("ğŸ“„ PDFåŸå§‹æ–‡å­—ï¼ˆå‰1000å­—ç¬¦ï¼‰")
                st.text_area("æå–çš„æ–‡å­—", full_text[:1000] + "..." if len(full_text) > 1000 else full_text, height=300)
            else:
                st.subheader("ğŸ“„ å­—å¹•å…§å®¹")
                if subtitles:
                    # é¡¯ç¤ºå­—å¹•åˆ—è¡¨
                    subtitle_df = pd.DataFrame([
                        {
                            "åºè™Ÿ": i+1,
                            "æ™‚é–“æˆ³": sub["timestamp"],
                            "å­—å¹•å…§å®¹": sub["text"]
                        }
                        for i, sub in enumerate(subtitles)
                    ])
                    st.dataframe(subtitle_df, use_container_width=True)
                    
                    # é¡¯ç¤ºå®Œæ•´å­—å¹•æ–‡æœ¬
                    st.write("---")
                    st.subheader("ğŸ“ å®Œæ•´å­—å¹•æ–‡æœ¬")
                    full_subtitle_text = "\n\n".join([
                        f"{sub['timestamp']}\n{sub['text']}"
                        for sub in subtitles
                    ])
                    st.text_area("å­—å¹•å…§å®¹", full_subtitle_text, height=400)
                else:
                    st.info("â„¹ï¸ æ²’æœ‰æå–åˆ°å­—å¹•å…§å®¹")

# é¡¯ç¤ºè©åº«å…§å®¹
st.write("---")
st.subheader("ğŸ“š ç•¶å‰è©åº«å…§å®¹")
if not termbase.empty:
    # éæ¿¾é¸é …
    status_filter = st.selectbox("æŒ‰ç‹€æ…‹éæ¿¾", ["å…¨éƒ¨", "å·²ç¢ºèª", "æ–°å¢å¾…ç¢ºèª"])
    if status_filter != "å…¨éƒ¨":
        filtered_termbase = termbase[termbase["status"] == status_filter]
    else:
        filtered_termbase = termbase
    
    # é¡¯ç¤ºè©åº«è¡¨æ ¼
    st.dataframe(filtered_termbase, use_container_width=True)
    
    # ä¸‹è¼‰é¸é …
    csv = filtered_termbase.to_csv(index=False)
    st.download_button(
        label="ğŸ“¥ ä¸‹è¼‰è©åº« (CSV)",
        data=csv,
        file_name=f"termbase_{status_filter}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv"
    )
else:
    st.info("â„¹ï¸ è©åº«ç‚ºç©º")
